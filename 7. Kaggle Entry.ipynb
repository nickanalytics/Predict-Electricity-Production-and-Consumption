{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0331550",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T17:12:41.261208Z",
     "iopub.status.busy": "2024-02-28T17:12:41.260698Z",
     "iopub.status.idle": "2024-02-28T17:12:45.992663Z",
     "shell.execute_reply": "2024-02-28T17:12:45.991248Z"
    },
    "papermill": {
     "duration": 4.74398,
     "end_time": "2024-02-28T17:12:45.995867",
     "exception": false,
     "start_time": "2024-02-28T17:12:41.251887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "from numpy import median\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "import enefit\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aca6418e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T17:12:46.012859Z",
     "iopub.status.busy": "2024-02-28T17:12:46.011771Z",
     "iopub.status.idle": "2024-02-28T17:12:46.018009Z",
     "shell.execute_reply": "2024-02-28T17:12:46.016885Z"
    },
    "papermill": {
     "duration": 0.017608,
     "end_time": "2024-02-28T17:12:46.020713",
     "exception": false,
     "start_time": "2024-02-28T17:12:46.003105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e5a280",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T17:12:46.035277Z",
     "iopub.status.busy": "2024-02-28T17:12:46.034881Z",
     "iopub.status.idle": "2024-02-28T17:12:46.041889Z",
     "shell.execute_reply": "2024-02-28T17:12:46.040543Z"
    },
    "papermill": {
     "duration": 0.017666,
     "end_time": "2024-02-28T17:12:46.044707",
     "exception": false,
     "start_time": "2024-02-28T17:12:46.027041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    calendar = pd.read_csv('/kaggle/input/my-data/calendar.csv', keep_default_na=False)\n",
    "    gps_point = pd.read_csv('/kaggle/input/my-data/gps_point.csv', keep_default_na=False)\n",
    "    code_list = pd.read_csv('/kaggle/input/my-data/code_list.csv', keep_default_na=False)\n",
    "    return calendar, gps_point, code_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6db2cc84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T17:12:46.060282Z",
     "iopub.status.busy": "2024-02-28T17:12:46.059551Z",
     "iopub.status.idle": "2024-02-28T17:12:46.068226Z",
     "shell.execute_reply": "2024-02-28T17:12:46.066640Z"
    },
    "papermill": {
     "duration": 0.020266,
     "end_time": "2024-02-28T17:12:46.071505",
     "exception": false,
     "start_time": "2024-02-28T17:12:46.051239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_forecast(forecast, gps_point):\n",
    "    forecast['lat/long'] = forecast['latitude'].astype(str) + ',' + forecast['longitude'].astype(str)\n",
    "    selected_gps = gps_point.loc[gps_point['main_gps_county'] == 1, 'lat/long'].tolist()\n",
    "    forecast = forecast[forecast['lat/long'].isin(selected_gps)]\n",
    "    forecast = pd.merge(forecast, gps_point[['lat/long', 'county_code']], on='lat/long', how='left')\n",
    "    forecast['county_code'] = forecast['county_code'].astype('int64')\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85cca21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T17:12:46.086797Z",
     "iopub.status.busy": "2024-02-28T17:12:46.086305Z",
     "iopub.status.idle": "2024-02-28T17:12:46.094159Z",
     "shell.execute_reply": "2024-02-28T17:12:46.092793Z"
    },
    "papermill": {
     "duration": 0.018618,
     "end_time": "2024-02-28T17:12:46.096574",
     "exception": false,
     "start_time": "2024-02-28T17:12:46.077956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_dates(df):\n",
    "    for column in df.columns:\n",
    "        if column.lower().startswith('date') or '_date' in column.lower():\n",
    "            df[column] = df[column].astype('int64')\n",
    "            df[column] /= 10**9 \n",
    "            df[column] = df[column].apply(datetime.datetime.fromtimestamp)\n",
    "            df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a819219",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T17:12:46.111339Z",
     "iopub.status.busy": "2024-02-28T17:12:46.110837Z",
     "iopub.status.idle": "2024-02-28T17:12:46.119914Z",
     "shell.execute_reply": "2024-02-28T17:12:46.118794Z"
    },
    "papermill": {
     "duration": 0.019516,
     "end_time": "2024-02-28T17:12:46.122559",
     "exception": false,
     "start_time": "2024-02-28T17:12:46.103043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_columns_to_test(test):  \n",
    "    test['datetime_date'] = test['prediction_datetime'].dt.date\n",
    "    test['datetime_date'] = pd.to_datetime(test['datetime_date'])\n",
    "    test['hour'] = test['prediction_datetime'].dt.time\n",
    "    test['hour'] = pd.to_datetime(test['hour'], format='%H:%M:%S').dt.hour\n",
    "    test['hour'] = test['hour'].astype('int64')\n",
    "    test['year'] = test['prediction_datetime'].dt.year\n",
    "    test['month'] = test['prediction_datetime'].dt.month    \n",
    "    test['week_num'] = test['prediction_datetime'].dt.isocalendar().week\n",
    "    test['week_num'] = test['week_num'].astype('int64')\n",
    "    test['day'] = test['prediction_datetime'].dt.day\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe5c9a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T17:12:46.139106Z",
     "iopub.status.busy": "2024-02-28T17:12:46.138665Z",
     "iopub.status.idle": "2024-02-28T17:12:46.144437Z",
     "shell.execute_reply": "2024-02-28T17:12:46.143232Z"
    },
    "papermill": {
     "duration": 0.0175,
     "end_time": "2024-02-28T17:12:46.147260",
     "exception": false,
     "start_time": "2024-02-28T17:12:46.129760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_columns_to_gas(gas):\n",
    "    gas['mwh_mean'] = gas[['lowest_price_per_mwh', 'highest_price_per_mwh']].mean(axis=1)\n",
    "    return gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee30c2a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T17:12:46.162828Z",
     "iopub.status.busy": "2024-02-28T17:12:46.162418Z",
     "iopub.status.idle": "2024-02-28T17:12:46.168164Z",
     "shell.execute_reply": "2024-02-28T17:12:46.166911Z"
    },
    "papermill": {
     "duration": 0.017145,
     "end_time": "2024-02-28T17:12:46.171098",
     "exception": false,
     "start_time": "2024-02-28T17:12:46.153953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_days_to_dates(df, date_column, days):\n",
    "    df[date_column] = df[date_column] + pd.Timedelta(days=days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a07ef102",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T17:12:46.186194Z",
     "iopub.status.busy": "2024-02-28T17:12:46.185770Z",
     "iopub.status.idle": "2024-02-28T17:12:46.191635Z",
     "shell.execute_reply": "2024-02-28T17:12:46.190373Z"
    },
    "papermill": {
     "duration": 0.0173,
     "end_time": "2024-02-28T17:12:46.194988",
     "exception": false,
     "start_time": "2024-02-28T17:12:46.177688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_code(row):\n",
    "    return (row['county'] * 7) + (row['product_type'] * 2) + row['is_business']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "417899b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T17:12:46.217586Z",
     "iopub.status.busy": "2024-02-28T17:12:46.217058Z",
     "iopub.status.idle": "2024-02-28T17:12:46.225021Z",
     "shell.execute_reply": "2024-02-28T17:12:46.223895Z"
    },
    "papermill": {
     "duration": 0.022808,
     "end_time": "2024-02-28T17:12:46.228057",
     "exception": false,
     "start_time": "2024-02-28T17:12:46.205249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_outliers(elec):\n",
    "    median_val = elec['euros_per_mwh'].median()\n",
    "    \n",
    "    # Calculate Median Absolute Deviation (MAD)\n",
    "    mad_val = np.median(np.abs(elec['euros_per_mwh'] - median_val))\n",
    "    \n",
    "    # Calculate z-score using MAD\n",
    "    elec['z_score'] = 0.6745 * ((elec['euros_per_mwh'] - median_val) / mad_val)\n",
    "\n",
    "    # Define threshold for outliers\n",
    "    threshold = 6\n",
    "    \n",
    "    # Identify outliers based on z-score\n",
    "    outliers = abs(elec['z_score']) > threshold\n",
    "    \n",
    "    # Replace outliers with NaN\n",
    "    elec.loc[outliers, 'euros_per_mwh'] = np.nan\n",
    "    \n",
    "    # Fill NaN values with the previous non-null value (forward fill)\n",
    "    elec['euros_per_mwh'] = elec['euros_per_mwh'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ab11f5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T17:12:46.246589Z",
     "iopub.status.busy": "2024-02-28T17:12:46.245591Z",
     "iopub.status.idle": "2024-02-28T17:12:46.258028Z",
     "shell.execute_reply": "2024-02-28T17:12:46.256805Z"
    },
    "papermill": {
     "duration": 0.02386,
     "end_time": "2024-02-28T17:12:46.260822",
     "exception": false,
     "start_time": "2024-02-28T17:12:46.236962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_dataframes(test, elec, gas, client, forecast, calendar):\n",
    "    # Merge 'elec' DataFrame into 'test'\n",
    "    test = test.merge(elec, left_on='prediction_datetime', right_on='forecast_date', how='left', suffixes=('', '_elec'))\n",
    "    \n",
    "    # Merge 'gas' DataFrame into 'test'\n",
    "    test = test.merge(gas, left_on='datetime_date', right_on='forecast_date', how='left', suffixes=('', '_gas'))\n",
    "    \n",
    "    # Merge specific columns from 'client' DataFrame into 'test'\n",
    "    test = pd.merge(test, client[['date', 'code', 'eic_count', 'installed_capacity']],\n",
    "                    left_on=['datetime_date', 'code'],\n",
    "                    right_on=['date', 'code'],\n",
    "                    how='left',\n",
    "                    suffixes=('_test_solar_', '_client'))\n",
    "   \n",
    "    # Merge 'forecast' DataFrame into 'test'\n",
    "    test = pd.merge(test, forecast,\n",
    "                    left_on=['prediction_datetime', 'county'],\n",
    "                    right_on=['forecast_datetime', 'county_code'],\n",
    "                    how='left',\n",
    "                    suffixes=('_forecast_weather_', '_forecast_weather2_'))\n",
    "    \n",
    "    # Merge specific columns from 'calendar' DataFrame into 'test'\n",
    "    test = pd.merge(test, calendar[['date', 'working_day']], left_on='datetime_date', right_on='date', how='left')\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7866962b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T17:12:46.278635Z",
     "iopub.status.busy": "2024-02-28T17:12:46.277900Z",
     "iopub.status.idle": "2024-02-28T17:12:46.288873Z",
     "shell.execute_reply": "2024-02-28T17:12:46.287569Z"
    },
    "papermill": {
     "duration": 0.023654,
     "end_time": "2024-02-28T17:12:46.291499",
     "exception": false,
     "start_time": "2024-02-28T17:12:46.267845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_model_column(test, code_list):\n",
    "    # Load the list of codes from the CSV file\n",
    "    cd_list = code_list['code'].to_list()\n",
    "\n",
    "    # Iterate over rows in the 'test' DataFrame\n",
    "    for index, row in test.iterrows():\n",
    "        # Check if 'is_consumption' is 0\n",
    "        if row['is_consumption'] == 0:\n",
    "            test.at[index, 'model'] = 'catboost_model.pkl'\n",
    "        else:\n",
    "            # Check if 'code' is in the code list\n",
    "            if row['code'] in cd_list:\n",
    "                test.at[index, 'model'] = f'model_{row[\"code\"]}.pkl'\n",
    "\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfffdee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T17:12:46.306558Z",
     "iopub.status.busy": "2024-02-28T17:12:46.305781Z",
     "iopub.status.idle": "2024-02-28T17:12:46.318196Z",
     "shell.execute_reply": "2024-02-28T17:12:46.317139Z"
    },
    "papermill": {
     "duration": 0.022763,
     "end_time": "2024-02-28T17:12:46.320676",
     "exception": false,
     "start_time": "2024-02-28T17:12:46.297913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_test(test):\n",
    "    # Columns to keep\n",
    "    cols_to_keep = ['row_id', 'prediction_datetime', 'hour', 'week_num', 'working_day',\n",
    "                    'county', 'is_business', 'product_type', 'is_consumption', 'code',\n",
    "                    'prediction_unit_id', 'euros_per_mwh', 'mwh_mean', 'eic_count',\n",
    "                    'installed_capacity', 'temperature', 'dewpoint', 'snowfall',\n",
    "                    'cloudcover_total', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high',\n",
    "                    'surface_solar_radiation_downwards', 'year', 'month', 'day', 'model']\n",
    "    \n",
    "    # Filter the DataFrame to keep only the specified columns\n",
    "    test = test[cols_to_keep]\n",
    "    \n",
    "    # Convert columns to the required data types\n",
    "    test['eic_count'] = test['eic_count'].astype(float)\n",
    "    test['hour'] = test['hour'].astype('int64')\n",
    "    test['row_id'] = test['row_id'].astype('int64')\n",
    "    test['week_num'] = test['week_num'].astype('int64')\n",
    "    test['working_day'] = test['working_day'].astype('int64')\n",
    "    test['county'] = test['county'].astype('int64')\n",
    "    test['is_business'] = test['is_business'].astype('int64')\n",
    "    test['product_type'] = test['product_type'].astype('int64')\n",
    "    test['is_consumption'] = test['is_consumption'].astype('int64')\n",
    "    test['code'] = test['code'].astype('int64')\n",
    "    test['prediction_unit_id'] = test['prediction_unit_id'].astype('int64')\n",
    "    test['year'] = test['year'].astype('int64')\n",
    "    test['month'] = test['month'].astype('int64')\n",
    "    test['day'] = test['day'].astype('int64')\n",
    "            \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "537c60b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T17:12:46.336438Z",
     "iopub.status.busy": "2024-02-28T17:12:46.335604Z",
     "iopub.status.idle": "2024-02-28T17:12:46.348051Z",
     "shell.execute_reply": "2024-02-28T17:12:46.346792Z"
    },
    "papermill": {
     "duration": 0.023735,
     "end_time": "2024-02-28T17:12:46.351184",
     "exception": false,
     "start_time": "2024-02-28T17:12:46.327449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_predictions(test):\n",
    "    # Load the CatBoost model\n",
    "    model_path_lgb = '/kaggle/input/catboost-model/catboost_model.pkl'\n",
    "    with open(model_path_lgb, 'rb') as model_file:\n",
    "        catboost_model = pickle.load(model_file)\n",
    "        \n",
    "    sample_prediction = pd.DataFrame(columns=['row_id', 'target'])\n",
    "    \n",
    "    predictions = []\n",
    "\n",
    "    # Iterate over rows in the 'test' DataFrame\n",
    "    for index, row in test.iterrows():\n",
    "        # Check the value in 'model' column\n",
    "        if row['model'] == 'catboost_model.pkl':\n",
    "            # Use CatBoost prediction\n",
    "            data = row.drop(['row_id', 'prediction_datetime', 'model', 'is_consumption', 'is_business', 'prediction_unit_id'])\n",
    "            prediction = catboost_model.predict(data.values.reshape(1, -1))[0]  # Convert data to numpy array and reshape\n",
    "        else:\n",
    "            # Load the Gradient Boosting model indicated in 'model' column\n",
    "            model_file_name = row['model']\n",
    "            model_file_path = os.path.join('/kaggle/input/models/', model_file_name)\n",
    "            with open(model_file_path, 'rb') as model_file:\n",
    "                gradient_boosting_model = pickle.load(model_file)\n",
    "\n",
    "            # Prepare the features for Gradient Boosting model\n",
    "            data = np.array([row['temperature'], row['working_day'], row['hour']]).reshape(1, -1)\n",
    "\n",
    "            # Make predictions using Gradient Boosting model\n",
    "            prediction = gradient_boosting_model.predict(data)[0]  # Assuming prediction is a single value\n",
    "\n",
    "        # Append the prediction along with its row_id to the predictions list\n",
    "        predictions.append({'row_id': int(row['row_id']), 'target': prediction})\n",
    "\n",
    "    # Create a DataFrame from the predictions list\n",
    "    sample_prediction = pd.DataFrame(predictions)\n",
    "\n",
    "    return sample_prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ee34338",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T17:12:46.366345Z",
     "iopub.status.busy": "2024-02-28T17:12:46.365511Z",
     "iopub.status.idle": "2024-02-28T17:13:43.870335Z",
     "shell.execute_reply": "2024-02-28T17:13:43.869050Z"
    },
    "papermill": {
     "duration": 57.521644,
     "end_time": "2024-02-28T17:13:43.879251",
     "exception": false,
     "start_time": "2024-02-28T17:12:46.357607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "All processing completed.\n"
     ]
    }
   ],
   "source": [
    "env = enefit.make_env()\n",
    "iter_test = env.iter_test()\n",
    "for (test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices, sample_prediction) in iter_test:\n",
    "        \n",
    "        # Load additional data\n",
    "        calendar, gps_point, code_list = load_data()\n",
    "        \n",
    "        # Rename historical_weather to hist\n",
    "        hist = historical_weather.copy()\n",
    "\n",
    "        # Rename forecast_weather to forecast\n",
    "        forecast = forecast_weather.copy()\n",
    "\n",
    "        # Rename electricity_prices to elec\n",
    "        elec = electricity_prices.copy()\n",
    "\n",
    "        # Rename gas_prices to gas\n",
    "        gas = gas_prices.copy()\n",
    "        \n",
    "        # Preprocess forecast data\n",
    "        forecast = preprocess_forecast(forecast_weather, gps_point)\n",
    "                \n",
    "        # Apply date conversion to dataframes\n",
    "        client = convert_dates(client)\n",
    "        forecast = convert_dates(forecast)\n",
    "        historical_weather = convert_dates(historical_weather)\n",
    "        gas_prices = convert_dates(gas_prices)\n",
    "        electricity_prices = convert_dates(electricity_prices)\n",
    "        test = convert_dates(test)\n",
    "        \n",
    "        # Convert calendar and gps_point dates to datetime\n",
    "        calendar['date'] = pd.to_datetime(calendar['date'], errors='coerce')\n",
    "                \n",
    "        # Add date time related columns to test\n",
    "        add_columns_to_test(test)\n",
    "        \n",
    "        # Add mean mwh to gas\n",
    "        add_columns_to_gas(gas_prices)\n",
    "        \n",
    "        # Add code column to test and client\n",
    "        client['code'] = client.apply(generate_code, axis=1)\n",
    "        test['code'] = test.apply(generate_code, axis=1)\n",
    "        \n",
    "        # Move calendar dates of client, gas, elec\n",
    "        add_days_to_dates(client, 'date', 2)\n",
    "        add_days_to_dates(gas_prices, 'forecast_date', 1)\n",
    "        add_days_to_dates(electricity_prices, 'forecast_date', 1)\n",
    "        \n",
    "        # Resolve outliers in electricity_prices\n",
    "        handle_outliers(electricity_prices)\n",
    "        \n",
    "        # Merge dataframes\n",
    "        test = merge_dataframes(test, electricity_prices, gas_prices, client, forecast, calendar)\n",
    "        \n",
    "        # Add the 'model' column to the test dataframe\n",
    "        add_model_column(test, code_list)\n",
    "        \n",
    "        # Additional preprocessing of test\n",
    "        test = preprocess_test(test)\n",
    "                \n",
    "        # Make predictions\n",
    "        sample_prediction = make_predictions(test)\n",
    "        \n",
    "        # Submit predictions\n",
    "        env.predict(sample_prediction)\n",
    "        \n",
    "print(\"All processing completed.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7292407,
     "sourceId": 57236,
     "sourceType": "competition"
    },
    {
     "datasetId": 4417722,
     "sourceId": 7589464,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4486199,
     "sourceId": 7687678,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4505217,
     "sourceId": 7714428,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 67.144983,
   "end_time": "2024-02-28T17:13:45.012536",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-28T17:12:37.867553",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
